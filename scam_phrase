import re
import requests
import socket
import ssl
import datetime
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from typing import Dict, List

# === Global Constants === #

SCAM_PHRASES = [
    # Urgency
    "urgent action required", "act now or lose forever", "limited time offer", "expires today",
    "immediate response required", "time sensitive",
    
    # Prize scams
    "claim your prize", "congratulations! you have won", "you've been selected", "lottery winner",
    "cash prize", "free gift", "free money",
    
    # Phishing
    "confirm your personal information", "verify your account", "update your details",
    "confirm your identity", "security verification required", "account suspended",
    
    # Financial
    "wire transfer", "send money", "pay processing fee", "tax refund", "inheritance money", "investment opportunity",
    
    # Fake institutions
    "hsbc", "hm office", "google verification", "paypal security", "amazon security",
    "microsoft support", "apple support", "irs notice",
    
    # Action
    "click to unlock", "download now", "install software", "run this file", "enable macros",
    
    # Emotional manipulation
    "don't tell anyone", "confidential matter", "help me transfer money", "i am dying", "refugee", "widow"
]

SUSPICIOUS_DOMAINS = [
    "bit.ly", "tinyurl.com", "t.co", "goo.gl", "ow.ly",
    "secure-bank", "paypal-secure", "amazon-security", "microsoft-support",
    "apple-security", "google-verify", "facebook-security", "tk", "xyz"
]

LEGITIMATE_DOMAINS = [
    "google.com", "microsoft.com", "apple.com", "amazon.com", "paypal.com",
    "facebook.com", "twitter.com", "linkedin.com", "github.com",
    "stackoverflow.com", "wikipedia.org", "reddit.com", "youtube.com",
    "gmail.com", "outlook.com"
]


# === URL Structure Checker === #

def check_url_structure(url: str) -> Dict:
    try:
        parsed = urlparse(url)
        domain, path = parsed.netloc.lower(), parsed.path.lower()
        suspicious_score, issues = 0, []

        if len(domain) > 50:
            suspicious_score += 20
            issues.append("Unusually long domain name")

        if re.search(r'[0-9]{4,}', domain):
            suspicious_score += 15
            issues.append("Domain contains many numbers")

        if any(shortener in domain for shortener in SUSPICIOUS_DOMAINS):
            suspicious_score += 30
            issues.append("Uses URL shortening service")

        if domain.count('.') > 2:
            suspicious_score += 10
            issues.append("Multiple subdomains")

        for legit in LEGITIMATE_DOMAINS:
            if is_similar_domain(domain, legit):
                suspicious_score += 40
                issues.append(f"Domain similar to {legit}")

        suspicious_paths = ['login', 'verify', 'secure', 'update', 'confirm', 'account', 'billing', 'payment', 'suspended', 'locked']
        for pattern in suspicious_paths:
            if pattern in path:
                suspicious_score += 5
                issues.append(f"Suspicious path contains: {pattern}")

        return {
            'score': min(suspicious_score, 100),
            'issues': issues,
            'domain': domain,
            'is_shortener': any(shortener in domain for shortener in SUSPICIOUS_DOMAINS)
        }

    except Exception as e:
        return {'score': 50, 'issues': [f"URL parsing error: {e}"], 'domain': 'unknown'}


def is_similar_domain(d1: str, d2: str) -> bool:
    d1 = re.sub(r'\.(com|org|net|edu|gov|mil)$', '', d1)
    d2 = re.sub(r'\.(com|org|net|edu|gov|mil)$', '', d2)
    return calculate_similarity(d1, d2) > 0.8 and d1 != d2


def calculate_similarity(s1: str, s2: str) -> float:
    longer, shorter = (s1, s2) if len(s1) >= len(s2) else (s2, s1)
    if not longer:
        return 1.0
    return (len(longer) - levenshtein_distance(longer, shorter)) / len(longer)


def levenshtein_distance(s1: str, s2: str) -> int:
    if len(s1) < len(s2): return levenshtein_distance(s2, s1)
    if not s2: return len(s1)
    prev_row = list(range(len(s2) + 1))
    for i, c1 in enumerate(s1):
        curr_row = [i + 1]
        for j, c2 in enumerate(s2):
            insert, delete = prev_row[j + 1] + 1, curr_row[j] + 1
            substitute = prev_row[j] + (c1 != c2)
            curr_row.append(min(insert, delete, substitute))
        prev_row = curr_row
    return prev_row[-1]


# === SSL Certificate Checker === #

def check_ssl_certificate(url: str) -> Dict:
    try:
        parsed = urlparse(url)
        hostname, port = parsed.hostname, parsed.port or 443

        if parsed.scheme != 'https':
            return {'valid': False, 'issues': ['No HTTPS used'], 'score': 40}

        context = ssl.create_default_context()
        with socket.create_connection((hostname, port), timeout=10) as sock:
            with context.wrap_socket(sock, server_hostname=hostname) as ssock:
                cert = ssock.getpeercert()
                not_after = datetime.datetime.strptime(cert['notAfter'], '%b %d %H:%M:%S %Y %Z')
                days_until_expiry = (not_after - datetime.datetime.now()).days

                score, issues = 0, []
                if days_until_expiry < 30:
                    score += 20
                    issues.append("SSL expires soon")

                subject = dict(x[0] for x in cert['subject'])
                cert_domain = subject.get('commonName', '')
                if hostname not in cert_domain and not cert_domain.startswith('*.'):
                    score += 30
                    issues.append("SSL certificate domain mismatch")

                issuer = dict(x[0] for x in cert['issuer']).get('organizationName', 'Unknown')

                return {
                    'valid': True,
                    'issues': issues,
                    'score': score,
                    'expires': not_after.strftime('%Y-%m-%d'),
                    'issuer': issuer
                }
    except Exception as e:
        return {'valid': False, 'issues': [f'SSL check failed: {e}'], 'score': 30}


# === Content Scraper & Analyzer === #

def scrape_and_analyze_content(url: str) -> Dict:
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        resp = requests.get(url, headers=headers, timeout=15)
        resp.raise_for_status()

        soup = BeautifulSoup(resp.content, 'html.parser')
        for tag in soup(['script', 'style', 'noscript']):
            tag.decompose()

        text = re.sub(r'\s+', ' ', soup.get_text()).strip()

        return {
            'scam_phrases': detect_scam_phrases(text),
            'suspicious_elements': analyze_html_elements(soup),
            'text_analysis': analyze_text_patterns(text),
            'content_preview': text[:300] + '...' if len(text) > 300 else text
        }

    except Exception as e:
        return {
            'error': str(e),
            'scam_phrases': [],
            'suspicious_elements': [],
            'text_analysis': {},
            'content_preview': 'Could not load content'
        }


def detect_scam_phrases(text: str) -> List[str]:
    return [phrase for phrase in SCAM_PHRASES if phrase.lower() in text.lower()]


def analyze_html_elements(soup) -> List[str]:
    findings = []
    if soup.find_all('form', style=re.compile(r'display:\s*none')):
        findings.append("Hidden forms detected")
    for iframe in soup.find_all('iframe'):
        if any(domain in iframe.get('src', '') for domain in SUSPICIOUS_DOMAINS):
            findings.append("Suspicious iframe source")
    if soup.find('meta', attrs={'http-equiv': 'refresh'}):
        findings.append("Auto-redirect detected")
    for link in soup.find_all('a', href=True):
        if any(domain in link['href'] for domain in SUSPICIOUS_DOMAINS):
            findings.append("Links to suspicious domains")
    return findings


def analyze_text_patterns(text: str) -> Dict:
    lower = text.lower()
    urgency = sum(lower.count(word) for word in ['urgent', 'immediate', 'quickly', 'asap', 'now', 'today', 'expires'])
    money = sum(lower.count(word) for word in ['$', 'money', 'cash', 'prize', 'million', 'fee'])
    spelling_errors = count_spelling_errors(text)
    exclamations = text.count('!')

    return {
        'urgency_score': min(urgency * 10, 50),
        'money_focus_score': min(money * 5, 30),
        'grammar_score': min(spelling_errors * 3, 25),
        'excitement_score': min(exclamations * 2, 20)
    }


def count_spelling_errors(text: str) -> int:
    common_misspellings = ['recieve', 'seperate', 'occured', 'goverment', 'beleive',
                           'neccessary', 'begining', 'existance', 'maintainance']
    words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
    return sum(1 for word in words if word in common_misspellings)


# === Risk Calculation === #

def calculate_overall_risk(url_data: Dict, ssl_data: Dict, content_data: Dict) -> Dict:
    score = url_data.get('score', 0) + ssl_data.get('score', 0)
    factors = url_data.get('issues', []) + ssl_data.get('issues', [])

    phrases = content_data.get('scam_phrases', [])
    score += len(phrases) * 15
    if phrases:
        factors.append(f"Found {len(phrases)} scam phrases")

    elements = content_data.get('suspicious_elements', [])
    score += len(elements) * 10
    factors.extend(elements)

    text_scores = content_data.get('text_analysis', {})
    for key, val in text_scores.items():
        score += val
        if val > 15:
            factors.append(f"High {key.replace('_', ' ')}")

    if score >= 80:
        level, color = 'VERY HIGH', 'danger'
    elif score >= 60:
        level, color = 'HIGH', 'danger'
    elif score >= 40:
        level, color = 'MEDIUM', 'warning'
    elif score >= 20:
        level, color = 'LOW', 'info'
    else:
        level, color = 'MINIMAL', 'success'

    return {
        'score': min(score, 100),
        'level': level,
        'color': color,
        'factors': factors[:10]
    }


# === Entry Point === #

def comprehensive_scan(url: str) -> Dict:
    timestamp = datetime.datetime.now().isoformat()
    try:
        url_analysis = check_url_structure(url)
        ssl_analysis = check_ssl_certificate(url)
        content_analysis = scrape_and_analyze_content(url)
        risk = calculate_overall_risk(url_analysis, ssl_analysis, content_analysis)

        return {
            'url': url,
            'timestamp': timestamp,
            'analyses': {
                'url_structure': url_analysis,
                'ssl_certificate': ssl_analysis,
                'content': content_analysis
            },
            'risk_assessment': risk,
            'status': 'success'
        }

    except Exception as e:
        return {
            'url': url,
            'timestamp': timestamp,
            'status': 'error',
            'error': str(e),
            'risk_assessment': {
                'score': 50,
                'level': 'UNKNOWN',
                'color': 'secondary',
                'factors': ['Analysis failed due to error']
            }
        }
